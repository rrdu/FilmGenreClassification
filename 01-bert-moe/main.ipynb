{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4dedd46",
   "metadata": {},
   "source": [
    "# SBERT Encoder w/ MLP MoE for Multilabel Film Genre Classification\n",
    "\n",
    "Short test poses a unique challenge for many NLP models due to limited context length while retaining the difficulties of the nuances of structured languages. This model aims to use several methods to address each of these concerns individually. First, a trained autoencoder, SBERT, will be used to create a new hidden representation for the short text input sequences. Then for the diverse classification tasks several expert models will be trained to selectively specialize in specific genre embeddings. Simple MLP models are sufficient for mapping the smaller embeddings to output classes while keeping the full model simple to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2b21aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch.callbacks import BaseFinetuning\n",
    "from torchmetrics.classification import MultilabelF1Score\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787d36dc",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e5570ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Expert(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple Feed-Forward Network acting as a single 'Expert'.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class TopKRouter(nn.Module):\n",
    "    \"\"\"\n",
    "    Gating Network that selects the top-k experts for each input.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, num_experts, top_k=2):\n",
    "        super().__init__()\n",
    "        self.gate = nn.Linear(input_dim, num_experts)\n",
    "        self.top_k = top_k\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.gate(x)\n",
    "        top_k_vals, top_k_indices = torch.topk(logits, self.top_k, dim=1)\n",
    "        router_probs = F.softmax(top_k_vals, dim=1)\n",
    "        return router_probs, top_k_indices, logits\n",
    "\n",
    "class MoEClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    The Mixture of Experts Classification Head.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, num_classes, num_experts=4, top_k=2, expert_hidden_dim=128):\n",
    "        super().__init__()\n",
    "        self.num_experts = num_experts\n",
    "        self.top_k = top_k\n",
    "        \n",
    "        self.router = TopKRouter(input_dim, num_experts, top_k=top_k)\n",
    "        self.experts = nn.ModuleList([\n",
    "            Expert(input_dim, expert_hidden_dim, num_classes) \n",
    "            for _ in range(num_experts)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        router_probs, expert_indices, router_logits = self.router(x)\n",
    "        \n",
    "        final_output = torch.zeros(batch_size, self.experts[0].net[-1].out_features).to(x.device)\n",
    "        \n",
    "        for k in range(self.top_k):\n",
    "            selected_experts = expert_indices[:, k]\n",
    "            gate_weight = router_probs[:, k].unsqueeze(1)\n",
    "            \n",
    "            for expert_idx in range(self.num_experts):\n",
    "                mask = (selected_experts == expert_idx)\n",
    "                if mask.any():\n",
    "                    expert_input = x[mask]\n",
    "                    expert_output = self.experts[expert_idx](expert_input)\n",
    "                    final_output[mask] += gate_weight[mask] * expert_output\n",
    "                    \n",
    "        return final_output, router_logits\n",
    "\n",
    "class SBERT_MoE_Model(nn.Module):\n",
    "    \"\"\"\n",
    "    Full End-to-End Model: SBERT Encoder + MoE Classifier\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name='all-MiniLM-L6-v2', num_classes=5, num_experts=8, top_k=2):\n",
    "        super().__init__()\n",
    "        # 1. Initialize SBERT\n",
    "        self.sbert = SentenceTransformer(model_name)\n",
    "        \n",
    "        # 2. Capture device choice\n",
    "        target_device = self.sbert.device \n",
    "        \n",
    "        # 3. Initialize MoE head\n",
    "        embedding_dim = self.sbert.get_sentence_embedding_dimension()\n",
    "        self.moe_head = MoEClassifier(\n",
    "            input_dim=embedding_dim,\n",
    "            num_classes=num_classes,\n",
    "            num_experts=num_experts,\n",
    "            top_k=top_k\n",
    "        )\n",
    "        \n",
    "        # 4. Force MoE head to same device\n",
    "        self.moe_head.to(target_device)\n",
    "\n",
    "    def forward(self, text_input):\n",
    "        # Encode and Detach to treat embeddings as fixed features initially\n",
    "        features = self.sbert.encode(text_input, convert_to_tensor=True)\n",
    "        \n",
    "        # Important: Clone and detach to avoid \"Inference Tensor\" errors during backprop\n",
    "        # If unfreezing later, the optimizer handles the graph connection, \n",
    "        # but for the initial forward pass logic, this is safe.\n",
    "        features = features.clone().detach() \n",
    "        \n",
    "        logits, router_logits = self.moe_head(features)\n",
    "        return logits, router_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af1d544",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df84f6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoE_LightningModule(pl.LightningModule):\n",
    "    def __init__(self, model, num_classes, num_experts, learning_rate=1e-3, aux_loss_weight=0.1):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(ignore=['model'])\n",
    "        \n",
    "        # Organize modules\n",
    "        self.backbone = model.sbert\n",
    "        self.head = model.moe_head\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_experts = num_experts\n",
    "        self.aux_loss_weight = aux_loss_weight\n",
    "        \n",
    "        # Multi-Label Loss & Metrics\n",
    "        self.criterion = nn.BCEWithLogitsLoss()\n",
    "        self.val_f1 = MultilabelF1Score(num_labels=num_classes, threshold=0.5, average='micro')\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 1. Tokenize the raw text\n",
    "        # This converts [\"Hello world\"] into {'input_ids': ..., 'attention_mask': ...}\n",
    "        features = self.backbone.tokenize(x)\n",
    "        \n",
    "        # 2. Move inputs to the correct device (GPU/MPS)\n",
    "        # LightningModule provides self.device\n",
    "        features = {key: value.to(self.device) for key, value in features.items()}\n",
    "        \n",
    "        # 3. Pass through SBERT backbone\n",
    "        # calling .forward() or __call__() allows gradients to flow (unlike .encode())\n",
    "        out = self.backbone(features)\n",
    "        \n",
    "        # 4. Extract the sentence embedding\n",
    "        embeddings = out['sentence_embedding']\n",
    "        \n",
    "        # 5. Pass through MoE Head\n",
    "        return self.head(embeddings)\n",
    "\n",
    "    def _compute_load_balancing_loss(self, router_logits):\n",
    "        probs = F.softmax(router_logits, dim=1)\n",
    "        mean_probs = probs.mean(dim=0)\n",
    "        aux_loss = (mean_probs ** 2).sum() * self.num_experts\n",
    "        return aux_loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        texts, targets = batch\n",
    "        logits, router_logits = self(texts)\n",
    "        \n",
    "        cls_loss = self.criterion(logits, targets)\n",
    "        aux_loss = self._compute_load_balancing_loss(router_logits)\n",
    "        \n",
    "        total_loss = cls_loss + (self.aux_loss_weight * aux_loss)\n",
    "        \n",
    "        self.log(\"train_loss\", total_loss)\n",
    "        return total_loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        texts, targets = batch\n",
    "        \n",
    "        # Forward pass (we don't need router logits for validation metrics)\n",
    "        logits, _ = self(texts)\n",
    "        \n",
    "        # Calculate Loss\n",
    "        val_loss = self.criterion(logits, targets)\n",
    "        \n",
    "        # Update F1 Score\n",
    "        self.val_f1(logits, targets)\n",
    "        \n",
    "        # Log metrics so the Scheduler can find 'val_loss'\n",
    "        self.log(\"val_loss\", val_loss, prog_bar=True)\n",
    "        self.log(\"val_f1\", self.val_f1, prog_bar=True)\n",
    "\n",
    "    # def configure_optimizers(self):\n",
    "    #     # Filter ensures we don't crash trying to optimize frozen params\n",
    "    #     return torch.optim.AdamW(filter(lambda p: p.requires_grad, self.parameters()), lr=self.learning_rate)\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        # 1. Initialize Optimizer with ONLY the Head (Backbone added later by Callback)\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            self.head.parameters(), \n",
    "            lr=1e-3, \n",
    "            weight_decay=0.01\n",
    "        )\n",
    "        \n",
    "        # 2. Use ReduceLROnPlateau (Safe for dynamic unfreezing)\n",
    "        # It waits for 'val_loss' to stop improving, then lowers LR\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer,\n",
    "            mode='min',\n",
    "            factor=0.1,\n",
    "            patience=2\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": scheduler,\n",
    "                \"monitor\": \"val_loss\", # Required for ReduceLROnPlateau\n",
    "                \"interval\": \"epoch\",\n",
    "                \"frequency\": 1\n",
    "            }\n",
    "        }\n",
    "        \n",
    "class ProductionFinetuning(BaseFinetuning):\n",
    "    def __init__(self, unfreeze_at_epoch=2, backbone_lr=2e-5):\n",
    "        super().__init__()\n",
    "        self._unfreeze_at_epoch = unfreeze_at_epoch\n",
    "        self._backbone_lr = backbone_lr\n",
    "\n",
    "    def freeze_before_training(self, pl_module):\n",
    "        # Start with SBERT frozen\n",
    "        self.freeze(pl_module.backbone)\n",
    "\n",
    "    def finetune_function(self, pl_module, current_epoch, optimizer):\n",
    "        # When we hit the target epoch...\n",
    "        if current_epoch == self._unfreeze_at_epoch:\n",
    "            print(f\"\\n‚ùÑÔ∏è -> üî• Unfreezing SBERT Backbone at Epoch {current_epoch}\")\n",
    "            \n",
    "            # Unfreeze AND add to the optimizer with the Low LR\n",
    "            self.unfreeze_and_add_param_group(\n",
    "                modules=pl_module.backbone,\n",
    "                optimizer=optimizer,\n",
    "                lr=self._backbone_lr \n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410cecab",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ce8ae30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.texts = texts\n",
    "        self.labels = labels \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.texts[idx], self.labels[idx]\n",
    "\n",
    "def map_logits_to_labels(logits, class_names, threshold=0.5):\n",
    "    probs = torch.sigmoid(logits).cpu()\n",
    "    predictions_mask = (probs > threshold).numpy()\n",
    "    batch_results = []\n",
    "    for is_active in predictions_mask:\n",
    "        active_labels = [class_names[idx] for idx, val in enumerate(is_active) if val]\n",
    "        batch_results.append(active_labels)\n",
    "    return batch_results, probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5429a00",
   "metadata": {},
   "source": [
    "## Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "820b20b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üí° Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "/home/dodogama/anaconda3/envs/nlp-py310/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/logger_connector/logger_connector.py:76: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3060 Ti') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type                | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0 | backbone  | SentenceTransformer | 22.7 M | train\n",
      "1 | head      | MoEClassifier       | 200 K  | train\n",
      "2 | criterion | BCEWithLogitsLoss   | 0      | train\n",
      "3 | val_f1    | MultilabelF1Score   | 0      | train\n",
      "----------------------------------------------------------\n",
      "200 K     Trainable params\n",
      "22.7 M    Non-trainable params\n",
      "22.9 M    Total params\n",
      "91.656    Total estimated model params size (MB)\n",
      "34        Modules in train mode\n",
      "120       Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Training ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cec589a6c00d46a8a4163c5cae73dd7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dodogama/anaconda3/envs/nlp-py310/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:484: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "/home/dodogama/anaconda3/envs/nlp-py310/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:433: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/dodogama/anaconda3/envs/nlp-py310/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:433: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/dodogama/anaconda3/envs/nlp-py310/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:527: Found 120 module(s) in eval mode at the start of training. This may lead to unexpected behavior during training. If this is intentional, you can ignore this warning.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "177cafbbdb4e4e3a9f8867a006cc0de5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b5afce76c974a609c83f18821aaa3c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ac8f849c1744db78170a8a116ebe99c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b20933ec388b41dab47062accd28662e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚ùÑÔ∏è -> üî• Unfreezing SBERT Backbone at Epoch 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0bc0e7224824e56918c14ec2da4b694",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "670a53c0d0164e9ba766b780f4361192",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fc7868de8104d3f94bafbdd44341d9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "651f8b013518404b920d6d24a9625c1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "647cc1191bbc44b8bafa7d51bd058aa7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cdb01a3eba64a028071ced44f50f89a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "951617c21b3e454ab6d7c3eb03cd49b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Running Inference ---\n",
      "\n",
      "Input: The tech company bought a sports team.\n",
      "Predicted Labels: ['Finance', 'Sports', 'Tech']\n",
      "Raw Probabilities: [0.5, 0.5, 0.5, 0.49]\n",
      "\n",
      "Input: Urgent: The bank is collapsing.\n",
      "Predicted Labels: ['Finance', 'Urgent']\n",
      "Raw Probabilities: [0.5, 0.41, 0.49, 0.53]\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "CLASS_NAMES = [\"Finance\", \"Sports\", \"Tech\", \"Urgent\"]\n",
    "\n",
    "texts = [\n",
    "    \"Inflation data released this morning caused immediate volatility across global markets. The S&P 500 dropped 2% within minutes of the opening bell as traders reacted to the Federal Reserve's signaling of potentially higher interest rates for the remainder of Q4.\",\n",
    "    \"The new smart-jersey utilizes embedded bio-sensors to track player fatigue in real-time during the match. Coaches can now monitor heart rate variability and sprint speed directly from the sidelines, allowing for data-driven substitution decisions in the fourth quarter.\",\n",
    "    \"After months of negotiation, the star quarterback has agreed to a record-breaking 50 million dollar extension. This deal makes him the highest-paid player in league history and significantly impacts the team's salary cap for the upcoming trading season.\",\n",
    "    \"Benchmark tests for the new M3 processor show a 15% drop in multi-core performance compared to the previous generation. Thermal throttling appears to be the primary bottleneck, as the chip reaches 90 degrees Celsius under sustained heavy workloads like video rendering.\",\n",
    "    \"CRITICAL ALERT: Primary database cluster US-East-1 is currently unresponsive due to a cascading failure in the load balancer. Response times have spiked to 5000ms. Immediate engineering intervention is required to prevent a total service outage for enterprise customers.\",\n",
    "    \"Emergency board meeting required: The Q3 projections show a cash flow deficit that will impact payroll by next Friday. We need to approve the emergency line of credit immediately to ensure operations continue without interruption.\",\n",
    "]\n",
    "\n",
    "# Multi-hot labels (Float Tensors)\n",
    "labels = torch.tensor([\n",
    "    [1.0, 0.0, 0.0, 0.0], # Finance\n",
    "    [0.0, 1.0, 1.0, 0.0], # Tech + Sports\n",
    "    [1.0, 1.0, 0.0, 0.0], # Finance + Sports\n",
    "    [0.0, 0.0, 1.0, 0.0], # Tech\n",
    "    [0.0, 0.0, 1.0, 1.0], # Tech + Urgent\n",
    "    [1.0, 0.0, 0.0, 1.0], # Finance + Urgent\n",
    "])\n",
    "\n",
    "dataset = TextDataset(texts, labels)\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "# --- Initialize Models ---\n",
    "num_classes = len(CLASS_NAMES)\n",
    "backbone = SBERT_MoE_Model(num_classes=num_classes, num_experts=4, top_k=2)\n",
    "\n",
    "# Wrap in Lightning\n",
    "pl_module = MoE_LightningModule(\n",
    "    model=backbone, \n",
    "    num_classes=num_classes, \n",
    "    num_experts=4,\n",
    "    aux_loss_weight=0.1\n",
    ")\n",
    "\n",
    "# Callback to unfreeze SBERT after 1 epoch\n",
    "finetune_cb = ProductionFinetuning(unfreeze_at_epoch=3, backbone_lr=2e-5)\n",
    "\n",
    "# --- Train ---\n",
    "print(\"--- Starting Training ---\")\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=10,\n",
    "    accelerator=\"auto\",\n",
    "    callbacks=[finetune_cb],\n",
    "    enable_progress_bar=True,\n",
    "    log_every_n_steps=1 \n",
    ")\n",
    "\n",
    "# trainer.fit(pl_module, dataloader, dataloader)\n",
    "trainer.fit(\n",
    "    pl_module, \n",
    "    train_dataloaders=dataloader, \n",
    "    val_dataloaders=dataloader # Using same data for demo purposes\n",
    ")\n",
    "\n",
    "# --- Inference ---\n",
    "print(\"\\n--- Running Inference ---\")\n",
    "pl_module.eval()\n",
    "\n",
    "test_texts = [\n",
    "    \"The tech company bought a sports team.\", \n",
    "    \"Urgent: The bank is collapsing.\"\n",
    "]\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Run forward pass\n",
    "    logits, _ = pl_module(test_texts)\n",
    "    \n",
    "    # Map outputs\n",
    "    predicted_labels, probs = map_logits_to_labels(logits, CLASS_NAMES, threshold=0.5)\n",
    "    \n",
    "    for text, lbls, raw_probs in zip(test_texts, predicted_labels, probs):\n",
    "        print(f\"\\nInput: {text}\")\n",
    "        print(f\"Predicted Labels: {lbls}\")\n",
    "        print(f\"Raw Probabilities: {[round(p, 2) for p in raw_probs.tolist()]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d26611c",
   "metadata": {},
   "source": [
    "## Movie Genre Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b590f88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from imdb_arh_train.csv...\n",
      "Loading data from imdb_arh_val.csv...\n",
      "Loading data from imdb_arh_test.csv...\n",
      "--- Sanity Checks ---\n",
      "Num classes: 26\n",
      "First 5 classes: ['Action', 'Adult', 'Adventure', 'Animation', 'Biography']\n",
      "Train size: 67772\n",
      "Val size:   14523\n",
      "\n",
      "Sample Text Type: <class 'str'>\n",
      "Sample Label Type: <class 'torch.Tensor'>\n",
      "Sample Label Shape: torch.Size([26])\n",
      "Sample Label Dtype: torch.float32\n",
      "\n",
      "--- DataLoader Batch Check ---\n",
      "Batch Text Length: 32\n",
      "Batch Label Shape: torch.Size([32, 26])\n",
      "Shuffle is working (First elements differ).\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "\n",
    "class IMDBDataset(Dataset):\n",
    "    def __init__(self, data_dir_path, filename, class_names, text_col='description', label_col='genre_list'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data_dir_path (Path or str): Relative path to data directory.\n",
    "            filename (str): The CSV filename (e.g., 'train.csv').\n",
    "            class_names (list): List of valid classes (ORDER MATTERS).\n",
    "            text_col (str): Column name for text.\n",
    "            label_col (str): Column name for labels.\n",
    "        \"\"\"\n",
    "        # 1. Setup Path safely\n",
    "        self.data_path = Path(data_dir_path) / filename\n",
    "        \n",
    "        if not self.data_path.exists():\n",
    "            raise FileNotFoundError(f\"File not found at: {self.data_path.resolve()}\")\n",
    "            \n",
    "        print(f\"Loading data from {self.data_path.name}...\")\n",
    "        self.df = pd.read_csv(self.data_path)\n",
    "        \n",
    "        # 2. Process Text (Handle NaNs, ensure strings)\n",
    "        self.texts = self.df[text_col].fillna(\"\").astype(str).tolist()\n",
    "        \n",
    "        # 3. Process Labels (Multi-hot Encoding)\n",
    "        # Create map: \"Action\" -> 0, \"Drama\" -> 1, etc.\n",
    "        self.class_to_idx = {cls: i for i, cls in enumerate(class_names)}\n",
    "        self.num_classes = len(class_names)\n",
    "        self.labels = []\n",
    "        \n",
    "        # Track unseen genres for safety warning\n",
    "        unseen_genres = set()\n",
    "        \n",
    "        for genre_str in self.df[label_col]:\n",
    "            # Initialize zero vector [0.0, 0.0, ...]\n",
    "            label_vec = torch.zeros(self.num_classes, dtype=torch.float)\n",
    "            \n",
    "            if pd.notna(genre_str):\n",
    "                # Split \"Action, Drama\" -> [\"Action\", \"Drama\"]\n",
    "                # .strip() removes whitespace around words\n",
    "                current_genres = [g.strip() for g in str(genre_str).split(',')]\n",
    "                \n",
    "                for genre in current_genres:\n",
    "                    if genre in self.class_to_idx:\n",
    "                        idx = self.class_to_idx[genre]\n",
    "                        label_vec[idx] = 1.0\n",
    "                    else:\n",
    "                        # Track genres that don't match our class list\n",
    "                        unseen_genres.add(genre)\n",
    "            \n",
    "            self.labels.append(label_vec)\n",
    "\n",
    "        # 4. Warning System\n",
    "        # If this is the test set, and it has weird genres not in train, warn the user.\n",
    "        if unseen_genres:\n",
    "            print(f\"‚ö†Ô∏è  WARNING in {filename}: Found {len(unseen_genres)} genres not in the provided class list.\")\n",
    "            print(f\"   Examples of ignored genres: {list(unseen_genres)[:5]}\")\n",
    "            print(f\"   (These were ignored during label creation)\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.texts[idx], self.labels[idx]\n",
    "\n",
    "    @staticmethod\n",
    "    def discover_classes(data_dir_path, filename, label_col='genre_list'):\n",
    "        \"\"\"\n",
    "        Static utility to scan a CSV and return sorted unique class names.\n",
    "        Use this ONCE on your TRAINING set only.\n",
    "        \"\"\"\n",
    "        path = Path(data_dir_path) / filename\n",
    "        if not path.exists():\n",
    "             raise FileNotFoundError(f\"File not found at: {path.resolve()}\")\n",
    "\n",
    "        df = pd.read_csv(path)\n",
    "        \n",
    "        # Split by comma, explode list to rows, strip whitespace, find unique\n",
    "        genres = df[label_col].dropna().astype(str).str.split(',').explode().str.strip().unique()\n",
    "        \n",
    "        return sorted(list(genres))\n",
    "\n",
    "\n",
    "DATA_DIR = Path('../data/imdb_arh_trimmed')\n",
    "CLASS_NAMES = IMDBDataset.discover_classes(DATA_DIR, 'imdb_arh_train.csv')\n",
    "NUM_WORKERS = 4\n",
    "\n",
    "tr_ds = IMDBDataset(data_dir_path=DATA_DIR, filename='imdb_arh_train.csv', class_names=CLASS_NAMES)\n",
    "va_ds = IMDBDataset(data_dir_path=DATA_DIR, filename='imdb_arh_val.csv', class_names=CLASS_NAMES)\n",
    "te_ds = IMDBDataset(data_dir_path=DATA_DIR, filename='imdb_arh_test.csv', class_names=CLASS_NAMES)\n",
    "\n",
    "tr_loader = DataLoader(tr_ds, batch_size=32, num_workers=NUM_WORKERS, shuffle=True)\n",
    "va_loader = DataLoader(va_ds, batch_size=64, num_workers=NUM_WORKERS, shuffle=False)\n",
    "te_loader = DataLoader(te_ds, batch_size=64, num_workers=NUM_WORKERS, shuffle=False)\n",
    "\n",
    "print(\"--- Sanity Checks ---\")\n",
    "# 1. Check if classes were discovered\n",
    "print(f\"Num classes: {len(CLASS_NAMES)}\")\n",
    "print(f\"First 5 classes: {CLASS_NAMES[:5]}\")\n",
    "\n",
    "# 2. Check Dataset lengths\n",
    "print(f\"Train size: {len(tr_ds)}\")\n",
    "print(f\"Val size:   {len(va_ds)}\")\n",
    "\n",
    "# 3. Inspect a single sample\n",
    "sample_text, sample_label = tr_ds[0]\n",
    "\n",
    "print(f\"\\nSample Text Type: {type(sample_text)}\") # Should be <class 'str'>\n",
    "print(f\"Sample Label Type: {type(sample_label)}\") # Should be <class 'torch.Tensor'>\n",
    "print(f\"Sample Label Shape: {sample_label.shape}\") # Should be torch.Size([num_classes])\n",
    "print(f\"Sample Label Dtype: {sample_label.dtype}\") # Should be torch.float32\n",
    "\n",
    "print(\"\\n--- DataLoader Batch Check ---\")\n",
    "\n",
    "# Get one batch from the training loader\n",
    "batch_texts, batch_labels = next(iter(tr_loader))\n",
    "\n",
    "print(f\"Batch Text Length: {len(batch_texts)}\") # Should match batch_size (32)\n",
    "print(f\"Batch Label Shape: {batch_labels.shape}\") # Should be [32, num_classes]\n",
    "\n",
    "# Check if shuffling works (Compare first item of two different iterator calls)\n",
    "batch_texts_2, _ = next(iter(tr_loader))\n",
    "if batch_texts[0] != batch_texts_2[0]:\n",
    "    print(\"Shuffle is working (First elements differ).\")\n",
    "else:\n",
    "    print(\"Warning: Shuffle might not be working or dataset is very small.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2d9745",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üí° Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type                | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0 | backbone  | SentenceTransformer | 22.7 M | train\n",
      "1 | head      | MoEClassifier       | 212 K  | train\n",
      "2 | criterion | BCEWithLogitsLoss   | 0      | train\n",
      "3 | val_f1    | MultilabelF1Score   | 0      | train\n",
      "----------------------------------------------------------\n",
      "212 K     Trainable params\n",
      "22.7 M    Non-trainable params\n",
      "22.9 M    Total params\n",
      "91.701    Total estimated model params size (MB)\n",
      "34        Modules in train mode\n",
      "120       Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Training ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88d2752d610241d5ac47ffcbf057fa7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dodogama/anaconda3/envs/nlp-py310/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:527: Found 120 module(s) in eval mode at the start of training. This may lead to unexpected behavior during training. If this is intentional, you can ignore this warning.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b84c5d326434b108e6f002ab2116253",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- Initialize Models ---\n",
    "num_classes = len(CLASS_NAMES)\n",
    "backbone = SBERT_MoE_Model(num_classes=num_classes, num_experts=4, top_k=2)\n",
    "\n",
    "# Wrap in Lightning\n",
    "pl_module = MoE_LightningModule(\n",
    "    model=backbone, \n",
    "    num_classes=num_classes, \n",
    "    num_experts=10,\n",
    "    aux_loss_weight=0.1\n",
    ")\n",
    "\n",
    "# Callback to unfreeze SBERT after 1 epoch\n",
    "finetune_cb = ProductionFinetuning(unfreeze_at_epoch=3, backbone_lr=2e-5)\n",
    "\n",
    "# --- Train ---\n",
    "print(\"--- Starting Training ---\")\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=20,\n",
    "    accelerator=\"auto\",\n",
    "    callbacks=[finetune_cb],\n",
    "    enable_progress_bar=True,\n",
    "    log_every_n_steps=1 \n",
    ")\n",
    "\n",
    "trainer.fit(\n",
    "    pl_module, \n",
    "    train_dataloaders=tr_loader, \n",
    "    val_dataloaders=va_loader,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d53baca",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Running Inference on Test Loader ---\")\n",
    "pl_module.eval()\n",
    "\n",
    "# Optional: Move to CPU for inference loop if your GPU memory is tight, \n",
    "# though keeping it on device is faster if you have space.\n",
    "# pl_module.cpu() \n",
    "\n",
    "with torch.no_grad():\n",
    "    # enumerate to keep track of counts, limit to first 5 batches for clean output\n",
    "    for batch_idx, batch in enumerate(te_loader):\n",
    "        if batch_idx >= 5: break  # Stop after 5 batches to avoid spamming console\n",
    "\n",
    "        # Unpack batch (Dataset returns: text_string, label_tensor)\n",
    "        texts, true_label_tensors = batch\n",
    "        \n",
    "        # Run forward pass\n",
    "        # Note: inputs to SBERT model are usually just the list of strings\n",
    "        logits, _ = pl_module(texts)\n",
    "        \n",
    "        # Map outputs to class names\n",
    "        predicted_labels_batch, probs_batch = map_logits_to_labels(logits, CLASS_NAMES, threshold=0.5)\n",
    "        \n",
    "        # Loop through items in the batch (handles batch_size=1 or >1)\n",
    "        for i, (text, pred_lbls, probs) in enumerate(zip(texts, predicted_labels_batch, probs_batch)):\n",
    "            \n",
    "            # --- Helper: Decode the actual Ground Truth for comparison ---\n",
    "            true_indices = (true_label_tensors[i] == 1.0).nonzero(as_tuple=False).squeeze()\n",
    "            # Handle scalar/0-dim tensors if only 1 class exists\n",
    "            if true_indices.ndim == 0 and true_indices.numel() == 1:\n",
    "                true_indices = [true_indices.item()]\n",
    "            elif true_indices.ndim > 0:\n",
    "                true_indices = true_indices.tolist()\n",
    "            else: \n",
    "                true_indices = [] # No labels\n",
    "                \n",
    "            actual_lbls = [CLASS_NAMES[idx] for idx in true_indices]\n",
    "            # -------------------------------------------------------------\n",
    "\n",
    "            print(f\"\\n[Batch {batch_idx} - Sample {i}]\")\n",
    "            print(f\"Input Text:    {text[:80]}...\") # Truncate for readability\n",
    "            print(f\"PREDICTED:     {pred_lbls}\")\n",
    "            print(f\"GROUND TRUTH:  {actual_lbls}\")\n",
    "            \n",
    "            # Print only probabilities that are somewhat significant\n",
    "            significant_probs = {CLASS_NAMES[j]: round(p, 3) for j, p in enumerate(probs.tolist()) if p > 0.1}\n",
    "            print(f\"High Probs:    {significant_probs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca09386",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
