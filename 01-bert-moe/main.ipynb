{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4dedd46",
   "metadata": {},
   "source": [
    "# SBERT Encoder w/ MLP MoE for Multilabel Film Genre Classification\n",
    "\n",
    "Short test poses a unique challenge for many NLP models due to limited context length while retaining the difficulties of the nuances of structured languages. This model aims to use several methods to address each of these concerns individually. First, a trained autoencoder, SBERT, will be used to create a new hidden representation for the short text input sequences. Then for the diverse classification tasks several expert models will be trained to selectively specialize in specific genre embeddings. Simple MLP models are sufficient for mapping the smaller embeddings to output classes while keeping the full model simple to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "820b20b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "\n",
      "  | Name      | Type                | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0 | backbone  | SentenceTransformer | 22.7 M | train\n",
      "1 | head      | MoEClassifier       | 200 K  | train\n",
      "2 | criterion | BCEWithLogitsLoss   | 0      | train\n",
      "3 | val_f1    | MultilabelF1Score   | 0      | train\n",
      "----------------------------------------------------------\n",
      "200 K     Trainable params\n",
      "22.7 M    Non-trainable params\n",
      "22.9 M    Total params\n",
      "91.656    Total estimated model params size (MB)\n",
      "34        Modules in train mode\n",
      "120       Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Training ---\n",
      "Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 12.97it/s, v_num=11, val_loss=0.676, val_f1=0.762]\n",
      "â„ï¸ -> ðŸ”¥ Unfreezing SBERT Backbone at Epoch 3\n",
      "Epoch 14: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  8.03it/s, v_num=11, val_loss=0.514, val_f1=1.000]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=15` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  3.61it/s, v_num=11, val_loss=0.514, val_f1=1.000]\n",
      "\n",
      "--- Running Inference ---\n",
      "\n",
      "Input: The tech company bought a sports team.\n",
      "Predicted Labels: ['Finance', 'Sports']\n",
      "Raw Probabilities: [0.5, 0.55, 0.47, 0.39]\n",
      "\n",
      "Input: Urgent: The bank is collapsing.\n",
      "Predicted Labels: ['Finance', 'Urgent']\n",
      "Raw Probabilities: [0.54, 0.43, 0.49, 0.57]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch.callbacks import BaseFinetuning\n",
    "from torchmetrics.classification import MultilabelF1Score\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# ==========================================\n",
    "# 1. MODEL ARCHITECTURE (MoE Components)\n",
    "# ==========================================\n",
    "\n",
    "class Expert(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple Feed-Forward Network acting as a single 'Expert'.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class TopKRouter(nn.Module):\n",
    "    \"\"\"\n",
    "    Gating Network that selects the top-k experts for each input.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, num_experts, top_k=2):\n",
    "        super().__init__()\n",
    "        self.gate = nn.Linear(input_dim, num_experts)\n",
    "        self.top_k = top_k\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.gate(x)\n",
    "        top_k_vals, top_k_indices = torch.topk(logits, self.top_k, dim=1)\n",
    "        router_probs = F.softmax(top_k_vals, dim=1)\n",
    "        return router_probs, top_k_indices, logits\n",
    "\n",
    "class MoEClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    The Mixture of Experts Classification Head.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, num_classes, num_experts=4, top_k=2, expert_hidden_dim=128):\n",
    "        super().__init__()\n",
    "        self.num_experts = num_experts\n",
    "        self.top_k = top_k\n",
    "        \n",
    "        self.router = TopKRouter(input_dim, num_experts, top_k=top_k)\n",
    "        self.experts = nn.ModuleList([\n",
    "            Expert(input_dim, expert_hidden_dim, num_classes) \n",
    "            for _ in range(num_experts)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        router_probs, expert_indices, router_logits = self.router(x)\n",
    "        \n",
    "        final_output = torch.zeros(batch_size, self.experts[0].net[-1].out_features).to(x.device)\n",
    "        \n",
    "        for k in range(self.top_k):\n",
    "            selected_experts = expert_indices[:, k]\n",
    "            gate_weight = router_probs[:, k].unsqueeze(1)\n",
    "            \n",
    "            for expert_idx in range(self.num_experts):\n",
    "                mask = (selected_experts == expert_idx)\n",
    "                if mask.any():\n",
    "                    expert_input = x[mask]\n",
    "                    expert_output = self.experts[expert_idx](expert_input)\n",
    "                    final_output[mask] += gate_weight[mask] * expert_output\n",
    "                    \n",
    "        return final_output, router_logits\n",
    "\n",
    "class SBERT_MoE_Model(nn.Module):\n",
    "    \"\"\"\n",
    "    Full End-to-End Model: SBERT Encoder + MoE Classifier\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name='all-MiniLM-L6-v2', num_classes=5, num_experts=8, top_k=2):\n",
    "        super().__init__()\n",
    "        # 1. Initialize SBERT\n",
    "        self.sbert = SentenceTransformer(model_name)\n",
    "        \n",
    "        # 2. Capture device choice\n",
    "        target_device = self.sbert.device \n",
    "        \n",
    "        # 3. Initialize MoE head\n",
    "        embedding_dim = self.sbert.get_sentence_embedding_dimension()\n",
    "        self.moe_head = MoEClassifier(\n",
    "            input_dim=embedding_dim,\n",
    "            num_classes=num_classes,\n",
    "            num_experts=num_experts,\n",
    "            top_k=top_k\n",
    "        )\n",
    "        \n",
    "        # 4. Force MoE head to same device\n",
    "        self.moe_head.to(target_device)\n",
    "\n",
    "    def forward(self, text_input):\n",
    "        # Encode and Detach to treat embeddings as fixed features initially\n",
    "        features = self.sbert.encode(text_input, convert_to_tensor=True)\n",
    "        \n",
    "        # Important: Clone and detach to avoid \"Inference Tensor\" errors during backprop\n",
    "        # If unfreezing later, the optimizer handles the graph connection, \n",
    "        # but for the initial forward pass logic, this is safe.\n",
    "        features = features.clone().detach() \n",
    "        \n",
    "        logits, router_logits = self.moe_head(features)\n",
    "        return logits, router_logits\n",
    "\n",
    "# ==========================================\n",
    "# 2. LIGHTNING MODULE & CALLBACKS\n",
    "# ==========================================\n",
    "\n",
    "class MoE_LightningModule(pl.LightningModule):\n",
    "    def __init__(self, model, num_classes, num_experts, learning_rate=1e-3, aux_loss_weight=0.1):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(ignore=['model'])\n",
    "        \n",
    "        # Organize modules\n",
    "        self.backbone = model.sbert\n",
    "        self.head = model.moe_head\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_experts = num_experts\n",
    "        self.aux_loss_weight = aux_loss_weight\n",
    "        \n",
    "        # Multi-Label Loss & Metrics\n",
    "        self.criterion = nn.BCEWithLogitsLoss()\n",
    "        self.val_f1 = MultilabelF1Score(num_labels=num_classes, threshold=0.5, average='micro')\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 1. Tokenize the raw text\n",
    "        # This converts [\"Hello world\"] into {'input_ids': ..., 'attention_mask': ...}\n",
    "        features = self.backbone.tokenize(x)\n",
    "        \n",
    "        # 2. Move inputs to the correct device (GPU/MPS)\n",
    "        # LightningModule provides self.device\n",
    "        features = {key: value.to(self.device) for key, value in features.items()}\n",
    "        \n",
    "        # 3. Pass through SBERT backbone\n",
    "        # calling .forward() or __call__() allows gradients to flow (unlike .encode())\n",
    "        out = self.backbone(features)\n",
    "        \n",
    "        # 4. Extract the sentence embedding\n",
    "        embeddings = out['sentence_embedding']\n",
    "        \n",
    "        # 5. Pass through MoE Head\n",
    "        return self.head(embeddings)\n",
    "\n",
    "    def _compute_load_balancing_loss(self, router_logits):\n",
    "        probs = F.softmax(router_logits, dim=1)\n",
    "        mean_probs = probs.mean(dim=0)\n",
    "        aux_loss = (mean_probs ** 2).sum() * self.num_experts\n",
    "        return aux_loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        texts, targets = batch\n",
    "        logits, router_logits = self(texts)\n",
    "        \n",
    "        cls_loss = self.criterion(logits, targets)\n",
    "        aux_loss = self._compute_load_balancing_loss(router_logits)\n",
    "        \n",
    "        total_loss = cls_loss + (self.aux_loss_weight * aux_loss)\n",
    "        \n",
    "        self.log(\"train_loss\", total_loss)\n",
    "        return total_loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        texts, targets = batch\n",
    "        \n",
    "        # Forward pass (we don't need router logits for validation metrics)\n",
    "        logits, _ = self(texts)\n",
    "        \n",
    "        # Calculate Loss\n",
    "        val_loss = self.criterion(logits, targets)\n",
    "        \n",
    "        # Update F1 Score\n",
    "        self.val_f1(logits, targets)\n",
    "        \n",
    "        # Log metrics so the Scheduler can find 'val_loss'\n",
    "        self.log(\"val_loss\", val_loss, prog_bar=True)\n",
    "        self.log(\"val_f1\", self.val_f1, prog_bar=True)\n",
    "\n",
    "    # def configure_optimizers(self):\n",
    "    #     # Filter ensures we don't crash trying to optimize frozen params\n",
    "    #     return torch.optim.AdamW(filter(lambda p: p.requires_grad, self.parameters()), lr=self.learning_rate)\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        # 1. Initialize Optimizer with ONLY the Head (Backbone added later by Callback)\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            self.head.parameters(), \n",
    "            lr=1e-3, \n",
    "            weight_decay=0.01\n",
    "        )\n",
    "        \n",
    "        # 2. Use ReduceLROnPlateau (Safe for dynamic unfreezing)\n",
    "        # It waits for 'val_loss' to stop improving, then lowers LR\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer,\n",
    "            mode='min',\n",
    "            factor=0.1,\n",
    "            patience=2\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": scheduler,\n",
    "                \"monitor\": \"val_loss\", # Required for ReduceLROnPlateau\n",
    "                \"interval\": \"epoch\",\n",
    "                \"frequency\": 1\n",
    "            }\n",
    "        }\n",
    "        \n",
    "class ProductionFinetuning(BaseFinetuning):\n",
    "    def __init__(self, unfreeze_at_epoch=2, backbone_lr=2e-5):\n",
    "        super().__init__()\n",
    "        self._unfreeze_at_epoch = unfreeze_at_epoch\n",
    "        self._backbone_lr = backbone_lr\n",
    "\n",
    "    def freeze_before_training(self, pl_module):\n",
    "        # Start with SBERT frozen\n",
    "        self.freeze(pl_module.backbone)\n",
    "\n",
    "    def finetune_function(self, pl_module, current_epoch, optimizer):\n",
    "        # When we hit the target epoch...\n",
    "        if current_epoch == self._unfreeze_at_epoch:\n",
    "            print(f\"\\nâ„ï¸ -> ðŸ”¥ Unfreezing SBERT Backbone at Epoch {current_epoch}\")\n",
    "            \n",
    "            # Unfreeze AND add to the optimizer with the Low LR\n",
    "            self.unfreeze_and_add_param_group(\n",
    "                modules=pl_module.backbone,\n",
    "                optimizer=optimizer,\n",
    "                lr=self._backbone_lr \n",
    "            )\n",
    "\n",
    "class SBERTUnfreezeCallback(BaseFinetuning):\n",
    "    def __init__(self, unfreeze_at_epoch=2):\n",
    "        super().__init__()\n",
    "        self._unfreeze_at_epoch = unfreeze_at_epoch\n",
    "\n",
    "    def freeze_before_training(self, pl_module):\n",
    "        self.freeze(pl_module.backbone)\n",
    "\n",
    "    def finetune_function(self, pl_module, current_epoch, optimizer):\n",
    "        if current_epoch == self._unfreeze_at_epoch:\n",
    "            print(f\"\\nâ„ï¸ -> ðŸ”¥ Unfreezing SBERT Backbone at Epoch {current_epoch}\")\n",
    "            self.unfreeze_and_add_param_group(\n",
    "                modules=pl_module.backbone,\n",
    "                optimizer=optimizer,\n",
    "                lr=pl_module.learning_rate / 10.0 \n",
    "            )\n",
    "\n",
    "# ==========================================\n",
    "# 3. DATASET & UTILS\n",
    "# ==========================================\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.texts = texts\n",
    "        self.labels = labels \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.texts[idx], self.labels[idx]\n",
    "\n",
    "def map_logits_to_labels(logits, class_names, threshold=0.5):\n",
    "    probs = torch.sigmoid(logits).cpu()\n",
    "    predictions_mask = (probs > threshold).numpy()\n",
    "    batch_results = []\n",
    "    for is_active in predictions_mask:\n",
    "        active_labels = [class_names[idx] for idx, val in enumerate(is_active) if val]\n",
    "        batch_results.append(active_labels)\n",
    "    return batch_results, probs\n",
    "\n",
    "# ==========================================\n",
    "# 4. MAIN EXECUTION\n",
    "# ==========================================\n",
    "\n",
    "# --- Setup Data ---\n",
    "# Example: 4 Classes\n",
    "CLASS_NAMES = [\"Finance\", \"Sports\", \"Tech\", \"Urgent\"]\n",
    "\n",
    "texts = [\n",
    "    # 1. Finance (Market Report)\n",
    "    \"Inflation data released this morning caused immediate volatility across global markets. The S&P 500 dropped 2% within minutes of the opening bell as traders reacted to the Federal Reserve's signaling of potentially higher interest rates for the remainder of Q4.\",\n",
    "    \n",
    "    # 2. Tech + Sports (Wearable Tech)\n",
    "    \"The new smart-jersey utilizes embedded bio-sensors to track player fatigue in real-time during the match. Coaches can now monitor heart rate variability and sprint speed directly from the sidelines, allowing for data-driven substitution decisions in the fourth quarter.\",\n",
    "    \n",
    "    # 3. Sports + Finance (Contract News)\n",
    "    \"After months of negotiation, the star quarterback has agreed to a record-breaking 50 million dollar extension. This deal makes him the highest-paid player in league history and significantly impacts the team's salary cap for the upcoming trading season.\",\n",
    "    \n",
    "    # 4. Tech (Product Review)\n",
    "    \"Benchmark tests for the new M3 processor show a 15% drop in multi-core performance compared to the previous generation. Thermal throttling appears to be the primary bottleneck, as the chip reaches 90 degrees Celsius under sustained heavy workloads like video rendering.\",\n",
    "    \n",
    "    # 5. Tech + Urgent (Server Incident)\n",
    "    \"CRITICAL ALERT: Primary database cluster US-East-1 is currently unresponsive due to a cascading failure in the load balancer. Response times have spiked to 5000ms. Immediate engineering intervention is required to prevent a total service outage for enterprise customers.\",\n",
    "    \n",
    "    # 6. Finance + Urgent (Budget Crisis)\n",
    "    \"Emergency board meeting required: The Q3 projections show a cash flow deficit that will impact payroll by next Friday. We need to approve the emergency line of credit immediately to ensure operations continue without interruption.\"\n",
    "]\n",
    "\n",
    "# Multi-hot labels (Float Tensors)\n",
    "labels = torch.tensor([\n",
    "    [1.0, 0.0, 0.0, 0.0], # Finance\n",
    "    [0.0, 1.0, 1.0, 0.0], # Tech + Sports\n",
    "    [1.0, 1.0, 0.0, 0.0], # Finance + Sports\n",
    "    [0.0, 0.0, 1.0, 0.0], # Tech\n",
    "    [0.0, 0.0, 1.0, 1.0], # Tech + Urgent\n",
    "    [1.0, 0.0, 0.0, 1.0], # Finance + Urgent\n",
    "])\n",
    "\n",
    "dataset = TextDataset(texts, labels)\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "# --- Initialize Models ---\n",
    "num_classes = len(CLASS_NAMES)\n",
    "backbone = SBERT_MoE_Model(num_classes=num_classes, num_experts=4, top_k=2)\n",
    "\n",
    "# Wrap in Lightning\n",
    "pl_module = MoE_LightningModule(\n",
    "    model=backbone, \n",
    "    num_classes=num_classes, \n",
    "    num_experts=4,\n",
    "    aux_loss_weight=0.1\n",
    ")\n",
    "\n",
    "# Callback to unfreeze SBERT after 1 epoch\n",
    "# unfreeze_cb = SBERTUnfreezeCallback(unfreeze_at_epoch=1)\n",
    "finetune_cb = ProductionFinetuning(unfreeze_at_epoch=3, backbone_lr=2e-5)\n",
    "\n",
    "# --- Train ---\n",
    "print(\"--- Starting Training ---\")\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=15,\n",
    "    accelerator=\"auto\",\n",
    "    callbacks=[finetune_cb],\n",
    "    enable_progress_bar=True,\n",
    "    log_every_n_steps=1 \n",
    ")\n",
    "\n",
    "# trainer.fit(pl_module, dataloader, dataloader)\n",
    "trainer.fit(\n",
    "    pl_module, \n",
    "    train_dataloaders=dataloader, \n",
    "    val_dataloaders=dataloader # Using same data for demo purposes\n",
    ")\n",
    "\n",
    "# --- Inference ---\n",
    "print(\"\\n--- Running Inference ---\")\n",
    "pl_module.eval()\n",
    "\n",
    "test_texts = [\n",
    "    \"The tech company bought a sports team.\", \n",
    "    \"Urgent: The bank is collapsing.\"\n",
    "]\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Run forward pass\n",
    "    logits, _ = pl_module(test_texts)\n",
    "    \n",
    "    # Map outputs\n",
    "    predicted_labels, probs = map_logits_to_labels(logits, CLASS_NAMES, threshold=0.5)\n",
    "    \n",
    "    for text, lbls, raw_probs in zip(test_texts, predicted_labels, probs):\n",
    "        print(f\"\\nInput: {text}\")\n",
    "        print(f\"Predicted Labels: {lbls}\")\n",
    "        print(f\"Raw Probabilities: {[round(p, 2) for p in raw_probs.tolist()]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2d9745",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
